{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/11 23:15:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "try:\n",
    "    # Create a Spark session\n",
    "    spark = SparkSession.builder.appName(\"ParquetReader\").getOrCreate()\n",
    "\n",
    "    # Continue with the rest of your script...\n",
    "    # ...\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error:\", str(e))\n",
    "    # Print additional diagnostic information\n",
    "    print(\"Check Spark installation, resource availability, port availability, and firewall settings.\")\n",
    "    # Print Spark configuration for additional insights\n",
    "    print(\"Spark Configuration:\")\n",
    "    print(spark.sparkContext.getConf().getAll())\n",
    "\n",
    "    # Stop the Spark session if it was partially created\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------+--------------------+-----------------------+\n",
      "|       Hospital_Name|        City|Beds_Available|COVID_Beds_Available|NonCOVID_Beds_Available|\n",
      "+--------------------+------------+--------------+--------------------+-----------------------+\n",
      "|    Pune Hospital 74|        Pune|            73|                  18|                     55|\n",
      "|Hyderabad Hospita...|   Hyderabad|            86|                  78|                      8|\n",
      "|  Mumbai Hospital 30|      Mumbai|            69|                  30|                     39|\n",
      "|  Mumbai Hospital 56|      Mumbai|           166|                  73|                     93|\n",
      "|  Mumbai Hospital 66|      Mumbai|            56|                  48|                      8|\n",
      "|   Delhi Hospital 84|       Delhi|            97|                  87|                     10|\n",
      "|    Delhi Hospital 6|       Delhi|           162|                  93|                     69|\n",
      "|Bhubaneshwar Hosp...|Bhubaneshwar|            95|                  39|                     56|\n",
      "|Bhubaneshwar Hosp...|Bhubaneshwar|           189|                  14|                    175|\n",
      "|Bhubaneshwar Hosp...|Bhubaneshwar|           199|                  72|                    127|\n",
      "+--------------------+------------+--------------+--------------------+-----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Specify the paths to the Parquet files\n",
    "covid_data_path = \"data/covid_dataset.parquet\"\n",
    "hospital_data_path = \"data/hospital_data/hospital_data.parquet\"\n",
    "\n",
    "# Read the parquet file into the dataframes\n",
    "hospital_df = spark.read.parquet(hospital_data_path)\n",
    "\n",
    "hospital_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+--------------+-------+------------+\n",
      "|Patient_ID|Health_Condition|Vaccine_Status|Country|        City|\n",
      "+----------+----------------+--------------+-------+------------+\n",
      "|         1|        Critical|      Received|  India|      Bhopal|\n",
      "|         2|        Critical|      Received|  India|       Delhi|\n",
      "|         3|        Critical|      Received|  India|     Chennai|\n",
      "|         4|        Critical|      Received|  India|       Noida|\n",
      "|         5|        Critical|      Received|  India|Bhubaneshwar|\n",
      "|         6|        Critical|      Received|  India|      Bhopal|\n",
      "|         7|        Critical|      Received|  India|      Jhansi|\n",
      "|         8|        Critical|      Received|  India|      Mumbai|\n",
      "|         9|        Critical|      Received|  India|      Bhopal|\n",
      "|        10|        Critical|      Received|  India|     Chennai|\n",
      "+----------+----------------+--------------+-------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "covid_data_path = \"data/covid_dataset.parquet\"\n",
    "# Read the parquet file into the dataframes\n",
    "covid_df = spark.read.parquet(covid_data_path)\n",
    "\n",
    "covid_df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------------+--------------+-------+--------------------+--------------+--------------------+-----------------------+\n",
      "|        City|Patient_ID|Health_Condition|Vaccine_Status|Country|       Hospital_Name|Beds_Available|COVID_Beds_Available|NonCOVID_Beds_Available|\n",
      "+------------+----------+----------------+--------------+-------+--------------------+--------------+--------------------+-----------------------+\n",
      "|      Bhopal|         1|        Critical|      Received|  India|  Bhopal Hospital 53|            67|                  62|                      5|\n",
      "|      Bhopal|         1|        Critical|      Received|  India|  Bhopal Hospital 22|           143|                  42|                    101|\n",
      "|      Bhopal|         1|        Critical|      Received|  India|  Bhopal Hospital 62|            97|                  14|                     83|\n",
      "|       Delhi|         2|        Critical|      Received|  India|    Delhi Hospital 6|           162|                  93|                     69|\n",
      "|       Delhi|         2|        Critical|      Received|  India|   Delhi Hospital 84|            97|                  87|                     10|\n",
      "|       Noida|         4|        Critical|      Received|  India|   Noida Hospital 80|           114|                  59|                     55|\n",
      "|       Noida|         4|        Critical|      Received|  India|   Noida Hospital 50|            93|                  15|                     78|\n",
      "|       Noida|         4|        Critical|      Received|  India|   Noida Hospital 73|            59|                  57|                      2|\n",
      "|       Noida|         4|        Critical|      Received|  India|   Noida Hospital 27|            93|                  15|                     78|\n",
      "|Bhubaneshwar|         5|        Critical|      Received|  India|Bhubaneshwar Hosp...|            58|                  37|                     21|\n",
      "+------------+----------+----------------+--------------+-------+--------------------+--------------+--------------------+-----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "\n",
    "joined_df = covid_df.join(hospital_df,'City')\n",
    "joined_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write this dataframe into the csv file\n",
    "joined_df.write.csv(csv_output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae9c95233d295bbb10e0f31e195de03f3adfdc08e7ce88cffb4f48d3b3f2ae15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
